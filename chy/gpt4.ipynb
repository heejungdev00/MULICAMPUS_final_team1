{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"V28","mount_file_id":"1nWrpKe8pluoCaCs-K_vLFLKK_J_VZhqB","authorship_tag":"ABX9TyNMLRWu+pYR9vYARaRSsEzu"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"TPU"},"cells":[{"cell_type":"code","source":["pip install openpyxl"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UMhQmEt9A0mx","executionInfo":{"status":"ok","timestamp":1722303131406,"user_tz":-480,"elapsed":1576,"user":{"displayName":"조한영","userId":"02500137320203185766"}},"outputId":"a1ba05e1-8601-4c42-f46a-0dc7ffc52bae"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: openpyxl in /usr/local/lib/python3.10/dist-packages (3.1.5)\n","Requirement already satisfied: et-xmlfile in /usr/local/lib/python3.10/dist-packages (from openpyxl) (1.1.0)\n"]}]},{"cell_type":"code","execution_count":13,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pJiL3XNrADDr","executionInfo":{"status":"ok","timestamp":1722304320732,"user_tz":-480,"elapsed":581081,"user":{"displayName":"조한영","userId":"02500137320203185766"}},"outputId":"d5de95fc-4065-4a14-e870-a1c2ccbe982f"},"outputs":[{"output_type":"stream","name":"stderr","text":["The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n","The tokenizer class you load from this checkpoint is 'GPT2Tokenizer'. \n","The class this function is called from is 'PreTrainedTokenizerFast'.\n","Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFGPT2LMHeadModel: ['transformer.h.5.attn.masked_bias', 'transformer.h.3.attn.masked_bias', 'transformer.h.8.attn.masked_bias', 'transformer.h.10.attn.masked_bias', 'lm_head.weight', 'transformer.h.0.attn.masked_bias', 'transformer.h.2.attn.masked_bias', 'transformer.h.1.attn.masked_bias', 'transformer.h.9.attn.masked_bias', 'transformer.h.7.attn.masked_bias', 'transformer.h.4.attn.masked_bias', 'transformer.h.11.attn.masked_bias', 'transformer.h.6.attn.masked_bias']\n","- This IS expected if you are initializing TFGPT2LMHeadModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing TFGPT2LMHeadModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n","All the weights of TFGPT2LMHeadModel were initialized from the PyTorch model.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.\n"]},{"output_type":"stream","name":"stdout","text":["Loaded model: GPT2Config {\n","  \"_name_or_path\": \"skt/kogpt2-base-v2\",\n","  \"_num_labels\": 1,\n","  \"activation_function\": \"gelu_new\",\n","  \"architectures\": [\n","    \"GPT2LMHeadModel\"\n","  ],\n","  \"attn_pdrop\": 0.1,\n","  \"author\": \"Heewon Jeon(madjakarta@gmail.com)\",\n","  \"bos_token_id\": 0,\n","  \"created_date\": \"2021-04-28\",\n","  \"embd_pdrop\": 0.1,\n","  \"eos_token_id\": 1,\n","  \"gradient_checkpointing\": false,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"label2id\": {\n","    \"LABEL_0\": 0\n","  },\n","  \"layer_norm_epsilon\": 1e-05,\n","  \"license\": \"CC-BY-NC-SA 4.0\",\n","  \"model_type\": \"gpt2\",\n","  \"n_ctx\": 1024,\n","  \"n_embd\": 768,\n","  \"n_head\": 12,\n","  \"n_inner\": null,\n","  \"n_layer\": 12,\n","  \"n_positions\": 1024,\n","  \"pad_token_id\": 3,\n","  \"reorder_and_upcast_attn\": false,\n","  \"resid_pdrop\": 0.1,\n","  \"scale_attn_by_inverse_layer_idx\": false,\n","  \"scale_attn_weights\": true,\n","  \"summary_activation\": null,\n","  \"summary_first_dropout\": 0.1,\n","  \"summary_proj_to_labels\": true,\n","  \"summary_type\": \"cls_index\",\n","  \"summary_use_proj\": true,\n","  \"task_specific_params\": {\n","    \"text-generation\": {\n","      \"do_sample\": true,\n","      \"max_length\": 50\n","    }\n","  },\n","  \"transformers_version\": \"4.42.4\",\n","  \"use_cache\": true,\n","  \"vocab_size\": 51200\n","}\n","\n","Epoch 1/200\n","5/5 [==============================] - 70s 8s/step - loss: 0.0460 - val_loss: 0.0366\n","Epoch 2/200\n","5/5 [==============================] - 30s 6s/step - loss: 0.0260 - val_loss: 0.0287\n","Epoch 3/200\n","5/5 [==============================] - 31s 6s/step - loss: 0.0229 - val_loss: 0.0262\n","Epoch 4/200\n","5/5 [==============================] - 30s 6s/step - loss: 0.0214 - val_loss: 0.0259\n","Epoch 5/200\n","5/5 [==============================] - 30s 6s/step - loss: 0.0216 - val_loss: 0.0258\n","Epoch 6/200\n","5/5 [==============================] - 29s 6s/step - loss: 0.0205 - val_loss: 0.0258\n","Epoch 7/200\n","5/5 [==============================] - 30s 6s/step - loss: 0.0200 - val_loss: 0.0253\n","Epoch 8/200\n","5/5 [==============================] - 30s 6s/step - loss: 0.0198 - val_loss: 0.0251\n","Epoch 9/200\n","5/5 [==============================] - 30s 6s/step - loss: 0.0194 - val_loss: 0.0251\n","Epoch 10/200\n","5/5 [==============================] - 30s 6s/step - loss: 0.0187 - val_loss: 0.0249\n","Epoch 11/200\n","5/5 [==============================] - 30s 6s/step - loss: 0.0190 - val_loss: 0.0256\n","Epoch 12/200\n","5/5 [==============================] - 31s 6s/step - loss: 0.0189 - val_loss: 0.0248\n","Epoch 13/200\n","5/5 [==============================] - 30s 6s/step - loss: 0.0187 - val_loss: 0.0252\n","Epoch 14/200\n","5/5 [==============================] - 30s 6s/step - loss: 0.0184 - val_loss: 0.0257\n","Epoch 15/200\n","5/5 [==============================] - 30s 6s/step - loss: 0.0177 - val_loss: 0.0256\n","Epoch 16/200\n","5/5 [==============================] - 29s 6s/step - loss: 0.0172 - val_loss: 0.0263\n","Epoch 17/200\n","5/5 [==============================] - 29s 6s/step - loss: 0.0171 - val_loss: 0.0254\n","1/1 [==============================] - 1s 1s/step - loss: 0.0413\n","Test Loss: 0.04131656885147095\n","새로운 각도 차이: [16, -15, 23, -16, 24, -16, 12, -18]\n","생성된 피드백: 골세 더 펴 골 골려주 더세반을주세\"반을 펴 펴 구부 오른쪽\" 더요.요. 더\" 왼쪽\"세요. 펴\" 골 펴요.반은\"요.주\"반은 펴세 골주요.\" 펴 더주반을요. 구부 펴 왼쪽주 구부주 골 더시고 펴 오른쪽세 펴반은요.세주반은반을 더반은 더 구부요. 왼쪽 펴반을려\"\" 구부세세반은주 왼쪽세려 펴주 펴려요.려세 구부 골요.반을\"려반을반을세 오른쪽주려 더\n"]}],"source":["import tensorflow as tf\n","from transformers import PreTrainedTokenizerFast, TFGPT2LMHeadModel\n","import numpy as np\n","import json\n","import pandas as pd\n","from sklearn.model_selection import train_test_split\n","\n","# KoGPT2 모델과 토크나이저 로드\n","model_name = 'skt/kogpt2-base-v2'\n","tokenizer = PreTrainedTokenizerFast.from_pretrained(model_name, bos_token='</s>', eos_token='</s>', unk_token='<unk>', pad_token='<pad>', mask_token='<mask>')\n","model = TFGPT2LMHeadModel.from_pretrained(model_name, from_pt=True)\n","print(f\"Loaded model: {model.config}\")\n","\n","# 데이터 로드 및 전처리\n","feed = pd.read_excel(\"/content/drive/MyDrive/자연어 처리 학습 피드백.xlsx\", names=[\"각도차이\", \"피드백\"])\n","angle_differences = feed.loc[:, \"각도차이\"].apply(eval)\n","feedbacks = feed.loc[:, \"피드백\"]\n","\n","# 각도 차이 데이터 정규화\n","angle_differences_flat = [angle for angles in angle_differences for angle in angles]\n","angle_differences_normalized = (np.array(angle_differences_flat) - np.mean(angle_differences_flat)) / np.std(angle_differences_flat)\n","\n","# 정규화된 값을 원래 리스트 구조로 재구성\n","start = 0\n","angle_differences_normalized_list = []\n","for angles in angle_differences:\n","    end = start + len(angles)\n","    angle_differences_normalized_list.append(angle_differences_normalized[start:end].tolist())\n","    start = end\n","\n","# 데이터 준비\n","data = []\n","for angles, feedback in zip(angle_differences_normalized_list, feedbacks):\n","    angle_str = ' '.join([f\"{angle:.2f}\" for angle in angles])\n","    prompt = f\"각도 차이: {angle_str}\"\n","    item = {\n","        \"prompt\": prompt,\n","        \"completion\": feedback\n","    }\n","    data.append(item)\n","\n","# 데이터 증강 (간단한 예시)\n","# augmented_data = data.copy()\n","# for item in data:\n","#     augmented_item = item.copy()\n","#     augmented_item['completion'] = f\"다음은 자세에 대한 피드백입니다: {item['completion']}\"\n","#     augmented_data.append(augmented_item)\n","\n","# data = augmented_data\n","\n","# 데이터를 JSON 파일로 저장\n","with open('training_data.json', 'w', encoding='utf-8') as f:\n","    json.dump(data, f, ensure_ascii=False, indent=2)\n","\n","# 데이터 전처리 함수 (max_length 증가)\n","def preprocess_data(data, max_length=256):\n","    inputs = []\n","    labels = []\n","    for item in data:\n","        prompt = item['prompt']\n","        completion = item['completion']\n","        text = f\"{prompt} 피드백: {completion}\"\n","        inputs.append(text)\n","        labels.append(completion)\n","\n","    input_encodings = tokenizer(inputs, truncation=True, padding='max_length', max_length=max_length, return_tensors=\"tf\")\n","    input_ids = input_encodings['input_ids']\n","    attention_mask = input_encodings['attention_mask']\n","\n","    label_encodings = tokenizer(labels, truncation=True, padding='max_length', max_length=max_length, return_tensors=\"tf\")\n","    label_ids = label_encodings['input_ids']\n","\n","    prompt_lengths = [len(tokenizer.encode(input_text.split('피드백:')[0])) for input_text in inputs]\n","    mask = tf.sequence_mask(prompt_lengths, maxlen=max_length)\n","    label_ids = tf.where(mask, tf.constant(-100, shape=label_ids.shape), label_ids)\n","    label_ids = tf.where(label_ids == tokenizer.pad_token_id, -100, label_ids)\n","\n","    return input_ids, attention_mask, label_ids\n","\n","# 데이터 로드 및 전처리\n","with open('training_data.json', 'r', encoding='utf-8') as f:\n","    training_data = json.load(f)\n","\n","# 데이터 분할 (train, validation, test)\n","train_data, test_data = train_test_split(training_data, test_size=0.1, random_state=42)\n","train_data, val_data = train_test_split(train_data, test_size=0.1, random_state=42)\n","\n","train_inputs, train_attention_mask, train_labels = preprocess_data(train_data)\n","val_inputs, val_attention_mask, val_labels = preprocess_data(val_data)\n","test_inputs, test_attention_mask, test_labels = preprocess_data(test_data)\n","\n","# TensorFlow 데이터셋 생성\n","train_dataset = tf.data.Dataset.from_tensor_slices((\n","    {\"input_ids\": train_inputs, \"attention_mask\": train_attention_mask},\n","    {\"labels\": train_labels}\n",")).shuffle(len(train_data)).batch(16)\n","\n","val_dataset = tf.data.Dataset.from_tensor_slices((\n","    {\"input_ids\": val_inputs, \"attention_mask\": val_attention_mask},\n","    {\"labels\": val_labels}\n",")).batch(16)\n","\n","test_dataset = tf.data.Dataset.from_tensor_slices((\n","    {\"input_ids\": test_inputs, \"attention_mask\": test_attention_mask},\n","    {\"labels\": test_labels}\n",")).batch(16)\n","\n","# 학습률 스케줄러 및 옵티마이저 설정 (learning rate 조정)\n","lr_schedule = tf.keras.optimizers.schedules.CosineDecayRestarts(\n","    initial_learning_rate=1e-4,\n","    first_decay_steps=1000,\n","    t_mul=2.0,\n","    m_mul=0.9,\n","    alpha=0.1\n",")\n","optimizer = tf.keras.optimizers.Adam(learning_rate=lr_schedule)\n","\n","# 커스텀 손실 함수 (가중치 추가)\n","def custom_loss(labels, logits):\n","    loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n","    mask = tf.not_equal(labels, -100)\n","    active_loss = tf.reduce_sum(tf.cast(mask, tf.float32), axis=-1)\n","    labels = tf.boolean_mask(labels, mask)\n","    logits = tf.boolean_mask(logits, mask)\n","    loss = loss_fn(labels, logits)\n","    loss = tf.reduce_sum(loss) / tf.reduce_sum(active_loss)\n","    return loss\n","\n","# 모델 컴파일\n","model.compile(optimizer=optimizer, loss=custom_loss)\n","\n","# 조기 종료 콜백\n","early_stopping = tf.keras.callbacks.EarlyStopping(\n","    monitor='val_loss',\n","    patience=5,\n","    restore_best_weights=True\n",")\n","\n","# 모델 학습\n","history = model.fit(\n","    train_dataset,\n","    epochs=200,  # 에폭 수 증가\n","    validation_data=val_dataset,\n","    callbacks=[early_stopping]  # model_checkpoint 제거\n",")\n","\n","# 테스트 데이터로 평가\n","test_loss = model.evaluate(test_dataset)\n","print(f\"Test Loss: {test_loss}\")\n","\n","# 모델 저장 방식 변경\n","model.save_pretrained(\"./fine_tuned_kogpt2_tf\")\n","tokenizer.save_pretrained(\"./fine_tuned_kogpt2_tf\")\n","\n","# 새로운 각도 차이에 대한 피드백 생성 함수 (생성 파라미터 조정)\n","def generate_feedback(model, tokenizer, angle_differences, max_length=150):\n","    normalized_angles = (angle_differences - np.mean(angle_differences_flat)) / np.std(angle_differences_flat)\n","    angle_str = ' '.join([f\"{angle:.2f}\" for angle in normalized_angles])\n","    input_text = f\"다음 각도 차이에 대한 자세 피드백을 생성해주세요: {angle_str}\\n피드백:\"\n","    input_ids = tokenizer.encode(input_text, return_tensors='tf')\n","\n","    output = model.generate(\n","        input_ids,\n","        max_length=max_length,\n","        num_return_sequences=1,\n","        no_repeat_ngram_size=2,\n","        do_sample=True,\n","        temperature=0.7,\n","        top_k=50,\n","        top_p=0.95,\n","        eos_token_id=tokenizer.eos_token_id,\n","        pad_token_id=tokenizer.pad_token_id,\n","    )\n","\n","    generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n","    feedback = generated_text.split(\"피드백:\")[1].strip()\n","\n","    return feedback\n","\n","# 새로운 각도 차이에 대한 피드백 생성 예시\n","new_angle_differences = [16, -15, 23, -16, 24, -16, 12, -18]\n","feedback = generate_feedback(model, tokenizer, new_angle_differences)\n","print(f\"새로운 각도 차이: {new_angle_differences}\")\n","print(f\"생성된 피드백: {feedback}\")"]},{"cell_type":"code","source":[],"metadata":{"id":"pVqFvK_BAR6W"},"execution_count":null,"outputs":[]}]}